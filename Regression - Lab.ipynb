{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "m = X.shape[0]  \n",
    "n = X.shape[1] \n",
    "\n",
    "y = boston.target\n",
    " \n",
    "\n",
    "#standardize\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "#train_test plist 80% train 20% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "#adding intercept \n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    \n",
    "    def __init__(self, alpha=0.001, early_stopping = True, max_iter= 1000, \n",
    "            loss_old=10000, tol=1e-3, method=\"batch\",batch_size = 100):\n",
    "        self.alpha = alpha\n",
    "        self.early_stopping = early_stopping\n",
    "        self.max_iter = max_iter\n",
    "        self.loss_old = 10000\n",
    "        self.tol = tol\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    \n",
    "        \n",
    "    def predict(self,X):\n",
    "        return X @ self.theta\n",
    "    \n",
    "    def gradient(self,X, error):\n",
    "        return X.T @ error\n",
    "\n",
    "    def mse(self,yhat, y):\n",
    "        return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "             \n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        for j in range(self.max_iter):\n",
    "            \n",
    "            \n",
    "            if self.method == 'batch': #for batch method\n",
    "                X_train = X\n",
    "                y_train = y\n",
    "                \n",
    "                yhat = self.predict(X_train)\n",
    "                error = yhat - y_train\n",
    "                grad = self.gradient(X_train, error)\n",
    "                self.theta = self.theta - self.alpha * grad\n",
    "            \n",
    "            elif self.method == 'sto': #for stochastic method\n",
    "                a = np.arange(X.shape[0])\n",
    "                np.random.shuffle(a)\n",
    "                for i in a:\n",
    "                    X_stoc = X[i,:].reshape(1,-1)\n",
    "                    y_stoc = y[i]\n",
    "                    yhat = self.predict(X_stoc)\n",
    "                    error = yhat - y_stoc   \n",
    "                    grad = self.gradient(X_stoc, error)  \n",
    "                    self.theta = self.theta - self.alpha * grad\n",
    "                    \n",
    "            elif self.method == 'mini': #for mini-batch\n",
    "                \n",
    "                n_batches = np.ceil(X.shape[0]/self.batch_size)\n",
    "                last_batch = m%self.batch_size\n",
    "                \n",
    "                k = 0\n",
    "                for i in range(int(n_batches)):    \n",
    "                    if i != range(int(n_batches))[-1] : \n",
    "                        X_mini = X[k:k+self.batch_size]\n",
    "                        y_mini = y[k:k+self.batch_size]\n",
    "                        yhat = self.predict(X_mini)\n",
    "                        error = yhat - y_mini   \n",
    "                        grad = self.gradient(X_mini, error)  \n",
    "                        self.theta = self.theta - self.alpha * grad\n",
    "\n",
    "                        k=k+self.batch_size\n",
    "\n",
    "                    else:\n",
    "                        X_mini = X[k:k+last_batch]\n",
    "                        y_mini = y[k:k+last_batch]\n",
    "                        yhat = self.predict(X_mini)\n",
    "                        error = yhat - y_mini   \n",
    "                        grad = self.gradient(X_mini, error)  \n",
    "                        self.theta = self.theta - self.alpha * grad\n",
    "                        \n",
    "            if self.early_stopping == True: #for early stopping\n",
    "                yhat = self.predict(X)\n",
    "                new_loss = self.mse(yhat,y)\n",
    "                if np.abs(self.loss_old - new_loss) < self.tol:\n",
    "                    print('iteration stopped at = ' + str(j))\n",
    "                    break\n",
    "                else:\n",
    "                    continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LR(alpha = 0.001, method = \"batch\", max_iter = 100)\n",
    "model.fit(X_train,y_train)\n",
    "yhat = model.predict(X_test)\n",
    "mse = model.mse(yhat, y_test)\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.73328234, -0.49801977,  0.9520299 ,  0.31490166,  1.10203905,\n",
       "       -1.90293778,  2.39271912,  0.41779973, -2.94771463,  2.70586458,\n",
       "       -1.86232498, -2.13295683,  0.91024757, -4.91911424])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = LR(alpha = 0.001, method = \"sto\")\n",
    "model2.fit(X_train,y_train)\n",
    "yhat2 = model2.predict(X_test)\n",
    "mse = model2.mse(yhat2, y_test)\n",
    "print(\"MSE: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LR(alpha = 0.001, method = \"mini\")\n",
    "model3.fit(X_train,y_train)\n",
    "yhat3 = model3.predict(X_test)\n",
    "mse = model3.mse(yhat3, y_test)\n",
    "print(\"MSE: \", mse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
