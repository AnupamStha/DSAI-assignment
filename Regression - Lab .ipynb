{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the regression scratch code in our lecture such that:\n",
    "\n",
    "- Implement early stopping in which if the absolute difference between old loss and new loss does not exceed certain threshold, we abort the learning.\n",
    "\n",
    "- Implement options for stochastic gradient descent in which we use only one sample for training.  Make sure that sample does not repeat unless all samples are read at least once already.\n",
    "\n",
    "- Put everything into class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = load_boston()\n",
    "X = boston.data\n",
    "m = X.shape[0]  \n",
    "n = X.shape[1] \n",
    "\n",
    "y = boston.target\n",
    " \n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    " \n",
    "intercept = np.ones((X_train.shape[0], 1))\n",
    "X_train = np.concatenate((intercept, X_train), axis=1)\n",
    "intercept = np.ones((X_test.shape[0], 1))\n",
    "X_test = np.concatenate((intercept, X_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, alpha=0.001, max_iter= 10000, old_loss=9000, tol=1e-3, method=\"batch\", batch_size=20):\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.old_loss = old_loss\n",
    "        self.tol = tol\n",
    "        self.method = method\n",
    "        self.batch_size = batch_size \n",
    "    \n",
    "    def h_theta(self,X):\n",
    "        return X @ self.theta\n",
    "    \n",
    "    def gradient(self,X, error):\n",
    "        return X.T @ error\n",
    "\n",
    "    def mse(self,yhat, y):\n",
    "        return ((yhat - y)**2).sum() / yhat.shape[0]\n",
    "    \n",
    "    \n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "        rand_indx=[]\n",
    "       \n",
    "        for j in range(self.max_iter):\n",
    "            \n",
    "            if self.method == 'batch':\n",
    "                X_new = X\n",
    "                y_new = y\n",
    "                                          \n",
    "            elif self.method == 'sgd': \n",
    "                    j= np.random.randint(X.shape[0])\n",
    "                    for j in rand_indx:\n",
    "                        j= np.random.randint(X.shape[0])\n",
    "                \n",
    "                    X_new = X[j, :].reshape(1, -1)\n",
    "                    y_new = y[j]\n",
    "                    rand_indx.append(j)\n",
    "                    if len(rand_indx) == X.shape[0]:\n",
    "                        rand_indx = []\n",
    "                        \n",
    "            elif self.method == 'minibatch':\n",
    "                j= np.random.randint(X.shape[0])\n",
    "                X_new = X[j : j + self.batch_size]\n",
    "                y_new = y[j : j + self.batch_size]\n",
    "                \n",
    "                \n",
    "                \n",
    "            yhat = self.h_theta(X_new)\n",
    "            current_loss = self.mse(yhat, y_new)\n",
    "            difference = np.abs(current_loss - self.old_loss)\n",
    "            if difference < self.tol:\n",
    "                self.ite = j\n",
    "                break\n",
    "            \n",
    "            self.loss_old = current_loss  \n",
    "            error = yhat - y_new\n",
    "            grad = self.gradient(X_new, error)\n",
    "            self.theta = self.theta - self.alpha * grad\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE by batch method:  28.306454826995083\n"
     ]
    }
   ],
   "source": [
    "model1 = LinearRegression( alpha =0.0001,method = \"batch\")\n",
    "model1.fit(X_train,y_train)\n",
    "yhat = model1.h_theta(X_test)\n",
    "mse = model1.mse(yhat, y_test)\n",
    "print(\"MSE by batch method: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using sgd method:  28.267006707838497\n"
     ]
    }
   ],
   "source": [
    "model2 = LinearRegression(method='sgd')\n",
    "model2.fit(X_train, y_train)\n",
    "yhat = model2.h_theta(X_test)\n",
    "mse = model2.mse(yhat, y_test)\n",
    "print(\"MSE using sgd method: \", mse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE using minibatch mehod:  28.557418399800312\n"
     ]
    }
   ],
   "source": [
    "model3 = LinearRegression(method='minibatch')\n",
    "model3.fit(X_train, y_train)\n",
    "yhat = model3.h_theta(X_test)\n",
    "mse = model3.mse(yhat, y_test)\n",
    "print(\"MSE using minibatch mehod: \", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
